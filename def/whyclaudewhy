media-analyzer/backend/media_analyzer/urls.py
	why MEDIA_ROOT just in debug

	this to store media files which is obvious not ideal to store in the db (at least on postgres)


nginx
	why add cors straigth in nginx conf


	> is it correct to have cors configuration in nginx?

	● Yes and No - it depends on the service responsibility:

	  NGINX CORS is correct when:
	  - NGINX serves static media files (HLS videos)
	  - Browser needs to access video files cross-origin
	  - NGINX is the media server

	  Django CORS is correct when:
	  - Django serves API endpoints
	  - Angular makes API calls cross-origin
	  - Django is the application server

	  Your current setup needs BOTH:
	  - Django CORS: For Angular → Django API calls (/api/streams/)
	  - NGINX CORS: For Angular → NGINX media requests (/playlist.m3u8, /segment.ts)

	  Separation of concerns:
	  Angular (4200) → Django (8000)     # API calls - Django CORS
	  Angular (4200) → NGINX (8081)      # Video files - NGINX CORS

	  In production/K8s, you'd typically:
	  - Have one ingress controller handle all CORS
	  - Or use service mesh (Istio) for cross-origin policies

	  But for development, having CORS in both services is correct because they serve different content types to the browser.


on AI feature decide a model design, that Im not sure what actually matters

> can you explain why this model design?

● Looking at this model design, here's the reasoning:

  VideoAnalysis - Central hub that tracks each analysis run with metadata like processing time, confidence thresholds, and frame timestamps. This allows performance monitoring and
  result correlation.

  ObjectDetection - YOLO results with bounding boxes. Multiple detections can belong to one analysis (one frame can have many objects).

  SceneAnalysis - CLIP results for scene understanding, text detection, and embeddings. One-to-one with VideoAnalysis since CLIP analyzes the whole frame context.

  ProcessingQueue - Decouples video processing from real-time streaming. Allows configurable analysis types per segment (you might want only YOLO for some streams, CLIP+YOLO for
  others).

  Key design decisions:
  - Separation of concerns: YOLO (objects) vs CLIP (scenes/text) have different data structures
  - Flexible processing: Queue supports different AI model combinations per segment
  - Performance tracking: Processing times and confidence thresholds for optimization
  - Scalability: UUIDs and proper indexing for distributed processing
  - Real-time ready: Frame timestamps enable overlay synchronization

  The analysis_types JSON field in ProcessingQueue is particularly important - it lets you configure which AI models to run per stream (e.g., ['yolo', 'clip_scene'] for object detection
   + scene analysis).



still hardcoding stuff???

class TextDetectionAdapterFactory(AdapterFactory):                                                                                                                                     │ │
    """Factory for text detection adapters"""                                                                                                                                          │ │
    @staticmethod                                                                                                                                                                      │ │
    def create(provider_config):                                                                                                                                                       │ │
      provider_type = provider_config.get('provider_type')                                                                                                                           │ │
                                                                                                                                                                                     │ │
      if provider_type == 'gcp_vision':                                                                                                                                              │ │
          return GCPTextDetectionAdapter()                                                                                                                                           │ │
      elif provider_type == 'local_tesseract':                                                                                                                                       │ │
          return TesseractTextDetectionAdapter()                                                                                                                                     │ │
      else:                                                                                                                                                                          │ │
          raise ValueError(f"Unknown text detection provider: {provider_type}")             
